{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covariate Shift",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrdQHK3g7mYDArI2HlRsVe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lu-David/CovariateShift/blob/main/Covariate_Shift.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python implementation of 2014 Liu Paper\n",
        "\n",
        "https://proceedings.neurips.cc/paper/2014/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf"
      ],
      "metadata": {
        "id": "_2PBOjKyuBb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Classification"
      ],
      "metadata": {
        "id": "Wpc54G11wtVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "folder_path = '/content' # TODO Upload data files and / or specify path\n",
        "\n",
        "x_1 = scipy.io.loadmat(os.path.join(folder_path, 'x_1.mat'))['x_1']\n",
        "x_2 = scipy.io.loadmat(os.path.join(folder_path, 'x_2.mat'))['x_2']\n",
        "y_1 = np.transpose(scipy.io.loadmat(os.path.join(folder_path, 'y_1.mat'))['y_1'])\n",
        "y_2 = np.transpose(scipy.io.loadmat(os.path.join(folder_path, 'y_2.mat'))['y_2'])"
      ],
      "metadata": {
        "id": "S6gJmDSxtt81"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source mean \n",
        "mu_s = [6, 6] \n",
        "\n",
        "# source variance\n",
        "var_s = [[3, -2], [-2, 3]] \n",
        "\n",
        "# target mean\n",
        "mu_t = [7, 7] \n",
        "\n",
        "# target variance\n",
        "var_t = [[3, 2], [2, 3]] "
      ],
      "metadata": {
        "id": "QDH0YmX-0qlj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "DFAqFWgunKyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "params:\n",
        "  X_s : training data\n",
        "  y_s : training labels\n",
        "  r_st : source over target \n",
        "  r_ts : ratio of target / source density distribution for importance weighting method \n",
        "  lamb : regularization constant\n",
        "  lr : learning rate\n",
        "  max_itr : max iterations \n",
        "  min_gradient : min gradient before loop stops \n",
        "returns:\n",
        "  theta : weights \n",
        "\"\"\"\n",
        "def binaryRobustTrain(X_s, y_s, r_st, r_ts, lamb = 0.001, lr = 1, max_itr = 10000, min_gradient = 0.0001):\n",
        "  n_row, n_col = X_s.shape\n",
        "\n",
        "  # F : Features \n",
        "  F = np.concatenate((np.ones((n_row, 1)), X_s), axis=1) \n",
        "\n",
        "  # F_g : Features reweighted under target / source density\n",
        "  F_g = F * np.tile(r_ts, n_col + 1)\n",
        "\n",
        "  # P : predictions\n",
        "  P = np.zeros((n_row, 1)) \n",
        "\n",
        "  # S_g : velocity term in Adam? \n",
        "  S_g = np.ones((n_col + 1, 1)) * 1e-8\n",
        "\n",
        "  # theta : Weights\n",
        "  theta = np.ones((n_col + 1, 1)) \n",
        "\n",
        "  # l_0 : momentum\n",
        "  l_0 = 0\n",
        "\n",
        "  # l_1 : \n",
        "  l_1 = (1 + (1 + (4 * l_0 ** 2)) ** 0.5) / 2\n",
        "  delta_1 = 0\n",
        "\n",
        "  t = 1\n",
        "  while True:\n",
        "    t = t + 1\n",
        "\n",
        "    decay = np.sqrt(1000 / (1000 + t))\n",
        "\n",
        "    l_2 = (1 + (1 + (4 * l_1 ** 2)) ** 0.5) / 2\n",
        "    l_3 = (1 - l_1) / l_2\n",
        "\n",
        "    for i in range(n_row):\n",
        "      W = r_st[i]\n",
        "\n",
        "      # temp is evaluated so that you want your prediction theta * features to be as close to the real y_s. \n",
        "      # if prediction matches real value, then temp will be 1 for both classes\n",
        "      temp =  (np.dot(np.transpose(theta), np.transpose(F[i, :])) * y_s[i] * W)[0]   # why multiply by y_s here when you aren't multiplying by y_s in multiClassTrain? \n",
        "      temp_max = max(temp, -1 * temp)  \n",
        "      temp_min = min(temp, -1 * temp)\n",
        "\n",
        "      # P[i] : estimator that is bounded between 0 and 1? \n",
        "      # According to the paper, this is Theorem 2 or Part 5 I believe? \n",
        "      P[i] = np.exp(temp - temp_max - np.log(1 + np.exp(temp_min - temp_max)))\n",
        "\n",
        "    # G : Gradient \n",
        "    G = np.transpose(np.dot(np.transpose(P * y_s), F_g)) - np.transpose(np.dot(np.transpose(y_s), F_g)) + 2 * lamb * theta\n",
        "    if np.linalg.norm(G) < min_gradient:\n",
        "      print('Optimization stops by reaching minimum gradient.')\n",
        "      break\n",
        "\n",
        "    # updating velocity \n",
        "    S_g = S_g + G ** 2\n",
        "\n",
        "    # delta_2 : \n",
        "    delta_2 = theta - decay * lr * G / np.sqrt(S_g) \n",
        "    theta = (1 - l_3) * delta_2 + l_3 * delta_1\n",
        "    delta_1 = delta_2\n",
        "    l_1 = l_2\n",
        "\n",
        "    if t > max_itr:\n",
        "      print(\"Optimizination stops by reaching maximum iteration\")\n",
        "      break\n",
        "\n",
        "  return theta"
      ],
      "metadata": {
        "id": "iSTQ7Wyk0q49"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "mvn_s = multivariate_normal(mu_s, var_s)\n",
        "mvn_t = multivariate_normal(mu_t, var_t)\n",
        "\n",
        "# Because we have expert knowledge on mu and var for both source and target, \n",
        "# we can get the predicted probabilities for each data point under source and target distributions \n",
        "d_s = mvn_s.pdf(x_1)\n",
        "d_t = mvn_t.pdf(x_1)"
      ],
      "metadata": {
        "id": "21mGJZQQYjoU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RBA\n",
        "theta_1 = binaryRobustTrain(x_1, y_1, d_s / d_t, np.ones((x_1.shape[0], 1)))\n",
        "print(\"Weights\", theta_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIOqqxGmdjzw",
        "outputId": "51652775-3aeb-4c0f-b5d7-11840b0b9c0e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizination stops by reaching maximum iteration\n",
            "Weights [[11.91705313]\n",
            " [-1.00605443]\n",
            " [-1.0069282 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "theta_2 = binaryRobustTrain(x_1, y_1, np.ones((x_1.shape[0], 1)), np.ones((x_1.shape[0], 1)))\n",
        "print(\"Weights\", theta_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1dDLQsdGD8j",
        "outputId": "43527841-5c3b-4be3-b58e-c1522bd28fad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization stops by reaching minimum gradient.\n",
            "Weights [[12.35379291]\n",
            " [-1.04099695]\n",
            " [-1.0359673 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance Weighting\n",
        "r_ts = d_t / d_s\n",
        "theta_3 = binaryRobustTrain(x_1, y_1, np.ones((x_1.shape[0], 1)), r_ts.reshape(r_ts.shape[0], 1))\n",
        "print(\"Weights\", theta_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAggbFoMGZhH",
        "outputId": "9687d145-d50a-4c90-a35e-1001f9946b9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization stops by reaching minimum gradient.\n",
            "Weights [[ 9.40884544]\n",
            " [-0.76023811]\n",
            " [-0.85193742]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "vBAqpEPrnNCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binaryRobustTest(theta, X_t, y_t, r_st):\n",
        "  n_row, _ = X_t.shape\n",
        "\n",
        "  F = np.concatenate((np.ones((n_row, 1)), X_t), axis=1) \n",
        "\n",
        "  P = np.zeros((n_row, 1))\n",
        "  logloss = 0\n",
        "  prediction = np.zeros((n_row, 2))\n",
        "  for i in range(n_row):\n",
        "    W = r_st[i]\n",
        "    temp =  (np.dot(np.transpose(theta), np.transpose(F[i, :])) * y_t[i] * W)[0]      \n",
        "    temp_max = max(temp, -1 * temp)\n",
        "    temp_min = min(temp, -1 * temp)\n",
        "    P[i] = np.exp(temp - temp_max - np.log(1 + np.exp(temp_min - temp_max)))\n",
        "    logloss = logloss - np.log(P[i])\n",
        "\n",
        "    if y_t[i] == 1:\n",
        "      prediction[i] = [P[i], 1 - P[i]]\n",
        "    else:\n",
        "      prediction[i] = [1 - P[i], P[i]]\n",
        "\n",
        "  logloss = logloss / n_row / 0.6931\n",
        "  return logloss, prediction\n",
        "\n",
        "def computeAcc(pred, y):\n",
        "  n_row, n_class = pred.shape\n",
        "  \n",
        "  max_ind = np.argmax(pred, axis = 1)\n",
        "\n",
        "  summ = 0\n",
        "\n",
        "  if n_class == 2:\n",
        "    for i in range(n_row):\n",
        "      if max_ind[i] == 1 and y[i] == -1:\n",
        "        summ += 1\n",
        "      elif max_ind[i] == 0 and y[i] == 1:\n",
        "        summ += 1\n",
        "  else:\n",
        "    summ = sum(np.argmax(pred, axis = 1) == y_t - 1)\n",
        "\n",
        "  return summ / n_row"
      ],
      "metadata": {
        "id": "NBxGgX1tnNwQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_s = mvn_s.pdf(x_2)\n",
        "d_t = mvn_t.pdf(x_2)"
      ],
      "metadata": {
        "id": "BgUO9mgnrF7n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RBA\n",
        "logloss_1, pred_1 = binaryRobustTest(theta_1, x_2, y_2, d_s / d_t)\n",
        "print(logloss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkxdsV8hrWNa",
        "outputId": "edc9c95e-375e-4e72-a908-c09286c3083a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.82562143]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logloss_2, pred_2 = binaryRobustTest(theta_2, x_2, y_2, np.ones((x_1.shape[0], 1)))\n",
        "print(logloss_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX5hh4G7ryiH",
        "outputId": "290e596b-e1b3-44ac-c7d9-2ddff9e95130"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.78499119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logloss_3, pred_3 = binaryRobustTest(theta_3, x_2, y_2, np.ones((x_1.shape[0], 1)))\n",
        "print(logloss_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdvUgB8WtVR9",
        "outputId": "18036eef-6304-4954-9ee6-adb070472829"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.69593336]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(computeAcc(pred_1, y_2))\n",
        "print(computeAcc(pred_2, y_2))\n",
        "print(computeAcc(pred_3, y_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR7rPN4Htalw",
        "outputId": "8cf64440-ca09-42d8-b1cc-21ee5fc8d1a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.89\n",
            "0.9\n",
            "0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiClass Classification"
      ],
      "metadata": {
        "id": "3DfdxoUUwzw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "folder_path = '/content' # TODO Change this\n",
        "\n",
        "iris_train = scipy.io.loadmat(os.path.join(folder_path, 'iris_train.mat'))['iris_train']\n",
        "iris_test = scipy.io.loadmat(os.path.join(folder_path, 'iris_test.mat'))['iris_test']\n",
        "\n",
        "X_s = iris_train[:,0:-1]\n",
        "y_s = iris_train[:, -1]\n",
        "X_t = iris_test[:,0:-1]\n",
        "y_t = iris_test[:,-1]"
      ],
      "metadata": {
        "id": "f-x7il86yMPG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n"
      ],
      "metadata": {
        "id": "7nVajMrByMqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LRDensityEstimation(X_s, X_t, lambdas = [0.0625, 1, 16]):\n",
        "\n",
        "  np.random.seed(10) # seed set in matlab code as well for verification\n",
        "\n",
        "  ns_row, _ = X_s.shape\n",
        "  nt_row, _ = X_t.shape\n",
        "\n",
        "  inda_s = np.arange(ns_row)\n",
        "  inda_t = np.arange(nt_row)\n",
        "\n",
        "  nv_s = int(np.floor(0.2 * ns_row))\n",
        "  nv_t = int(np.floor(0.2 * nt_row))\n",
        "\n",
        "  indv_s = np.array([92, 3, 74, 87, 57, 26, 23, 85, 19, 10, 75, 103, 1, 54, 111, 64, 116, 30, 118, 71, 105, 14, 36]) - 1 # indv_s = np.random.permutation(ns_row)[:nv_s] \n",
        "\n",
        "  indv_t = np.array([22, 14, 31, 18, 15, 29]) - 1 # np.random.permutation(nt_row)[:nv_t]\n",
        "\n",
        "  indt_s = np.setdiff1d(inda_s, indv_s)\n",
        "  \n",
        "  indt_t = np.setdiff1d(inda_t, indv_t)\n",
        "\n",
        "  X_train = np.concatenate((X_s[indt_s, :], X_t[indt_t, :]))\n",
        "  X_valid = np.concatenate((X_s[indv_s, :], X_t[indv_t, :]))\n",
        "  \n",
        "  y_train = np.concatenate((np.ones((ns_row - nv_s, 1)), -1 * np.ones((nt_row - nv_t, 1)) ))\n",
        "  y_valid = np.concatenate((np.ones((nv_s, 1)), -1 * np.ones((nv_t, 1)) ))\n",
        "\n",
        "  rt_st = np.ones((ns_row + nt_row - nv_s - nv_t, 1))\n",
        "  rv_st = np.ones((nv_s + nv_t, 1))\n",
        "  \n",
        "  logloss = np.zeros((len(lambdas), 1))\n",
        "  for i, lamb in enumerate(lambdas):\n",
        "    theta = binaryRobustTrain(X_train, y_train, rt_st, rt_st, lamb=lamb,min_gradient=0.1)\n",
        "    _, pred = binaryRobustTest(theta, X_valid, y_valid, rv_st )\n",
        "    logloss[i] = (-sum(np.log(pred[:nv_s, 0])) - sum(np.log(pred[nv_s: nv_s + nv_t, 1]))) / (nv_s + nv_t) / 0.6931\n",
        "\n",
        "  ind_min = np.argmin(logloss)\n",
        "\n",
        "  X_train = np.concatenate((X_s, X_t))\n",
        "  y_train = np.concatenate((np.ones((ns_row, 1)), -1 * np.ones((nt_row, 1)) ))\n",
        "  r_st = np.ones((ns_row + nt_row, 1))\n",
        "\n",
        "  theta = binaryRobustTrain(X_train, y_train, r_st, r_st, lambdas[ind_min])\n",
        "  _, pred = binaryRobustTest(theta, X_train, y_train, r_st)\n",
        "\n",
        "  d_ss = pred[:ns_row, 0]\n",
        "  d_st = pred[:ns_row, 1]\n",
        "\n",
        "  d_ts = pred[ns_row:, 0]\n",
        "  d_tt = pred[ns_row:, 1]\n",
        "\n",
        "  print(\"Finish Density Estimation\")\n",
        "\n",
        "  return d_ss, d_st, d_ts, d_tt\n",
        "\n",
        "d_ss, d_st, d_ts, d_tt = LRDensityEstimation(X_s, X_t, [0.1, 1, 10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoMSv73QmAIA",
        "outputId": "fcaa6182-8a8e-4b78-87b1-1da777182633"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization stops by reaching minimum gradient.\n",
            "Optimization stops by reaching minimum gradient.\n",
            "Optimization stops by reaching minimum gradient.\n",
            "Optimization stops by reaching minimum gradient.\n",
            "Finish Density Estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "def multiClassRobustTrain(X_s, y_s, n_class, r_st, r_ts, lamb = 0.1, lr = 0.01, max_itr = 100000, min_gradient = 0.001):\n",
        "  n_row, n_col = X_s.shape\n",
        "\n",
        "  lamb = lamb.reshape(-1, 1)  \n",
        "  lamb = np.transpose(np.tile(lamb, n_class))\n",
        "  \n",
        "  F = X_s \n",
        "\n",
        "  F_g = F * np.tile(r_ts, n_col)\n",
        "\n",
        "  Y = lb.fit_transform(y_s)\n",
        "\n",
        "  P = np.zeros((n_row, n_class)) \n",
        "\n",
        "  S_g = np.ones((n_class, n_col)) * 1e-8\n",
        "\n",
        "  t = 1\n",
        "\n",
        "  theta = np.ones((n_class, n_col)) # not randomly assigned? \n",
        "\n",
        "  l_0 = 0\n",
        "\n",
        "  l_1 = (1 + (1 + (4 * l_0 ** 2)) ** 0.5) / 2\n",
        "  delta_1 = 0\n",
        "\n",
        "  while True:\n",
        "    t = t + 1\n",
        "    decay = np.sqrt(1000 / (1000 + t))\n",
        "    l_2 = (1 + (1 + (4 * l_1 ** 2)) ** 0.5) / 2\n",
        "    l_3 = (1 - l_1) / l_2\n",
        "\n",
        "    for i in range(n_row):\n",
        "      W = r_st[i]\n",
        "      temp =  np.dot(theta, np.transpose(F[i, :])) * W\n",
        "      temp = temp - np.max(temp)\n",
        "      sum_temp = sum(np.exp(temp))\n",
        "      P[i] = np.exp(temp - np.log(sum_temp))\n",
        "\n",
        "    G = np.dot(np.transpose(P) - np.transpose(Y), F_g) + 2 * lamb * theta\n",
        "\n",
        "    if np.linalg.norm(G) < min_gradient:\n",
        "      print('Optimization stops by reaching minimum gradient.')\n",
        "      break\n",
        "    elif t > max_itr:\n",
        "      print(\"Optimizination stops by reaching maximum iteration\")\n",
        "      break\n",
        "\n",
        "    S_g = S_g + G ** 2\n",
        "    delta_2 = theta - decay * lr * G / np.sqrt(S_g) \n",
        "    theta = np.dot((1 - l_3), delta_2) + np.dot(l_3, delta_1)\n",
        "    delta_1 = delta_2\n",
        "    l_1 = l_2\n",
        "    \n",
        "  return theta"
      ],
      "metadata": {
        "id": "2V9JCYprw13_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RBA\n",
        "\n",
        "ns_row, n_col = X_s.shape\n",
        "\n",
        "n_class = int(max(y_s))\n",
        "\n",
        "lamb = 2 * np.std(X_s, axis=0, ddof=1) / np.sqrt(ns_row)\n",
        "lamb[0] = 1\n",
        "\n",
        "theta_robust = multiClassRobustTrain(X_s, y_s, n_class, d_ss / d_st, np.ones((ns_row, 1)), lamb=lamb, lr = 1, min_gradient=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzyBcLk7j_de",
        "outputId": "8aab08dc-a7b9-4272-d58b-f55033f08ef1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization stops by reaching minimum gradient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "theta_lr = multiClassRobustTrain(X_s, y_s, n_class, np.ones((ns_row, 1)), np.ones((ns_row, 1)), lamb=lamb, lr = 1, min_gradient=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foUaf_XL3i9r",
        "outputId": "765d60c5-c5fa-47d8-8ea7-68c34711aa49"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization stops by reaching minimum gradient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance weighting\n",
        "r_ts = np.reshape(d_st / d_ss, (-1, 1))\n",
        "theta_iw = multiClassRobustTrain(X_s, y_s, n_class, np.ones((ns_row, 1)), r_ts, lamb=lamb, lr = 1, min_gradient=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5xA84O74TEc",
        "outputId": "622aaac6-4ed7-460e-ac7a-920cefcbdf49"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization stops by reaching minimum gradient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "pVpXrZe13nsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiClassRobustTest(theta, X_t, y_t, n_class, r_st):\n",
        "  n_row, _ = X_t.shape\n",
        "  F = X_t \n",
        "  P = np.zeros((n_row, n_class))\n",
        "  logloss = 0 \n",
        "\n",
        "  for i in range(n_row):\n",
        "      W = r_st[i]\n",
        "      temp =  np.dot(theta, np.transpose(F[i, :])) * W\n",
        "      temp = temp - np.max(temp)\n",
        "      sum_temp = sum(np.exp(temp))\n",
        "      P[i] = np.exp(temp - np.log(sum_temp))\n",
        "      logloss -= np.log(P[i, int(y_t[i]) - 1])\n",
        "\n",
        "  logloss = logloss / n_row / 0.6931\n",
        "  return logloss, P"
      ],
      "metadata": {
        "id": "pOode0kzyT4k"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logloss, pred = multiClassRobustTest(theta_robust, X_t, y_t, n_class, d_ts / d_tt)\n",
        "acc = computeAcc(pred, y_t)\n",
        "print(f'Acc is {acc} and logloss is {logloss} for robust method')"
      ],
      "metadata": {
        "id": "YEbPTr78l5X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00be0a1c-b463-485b-a49f-c90e10031988"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc is 1.0 and logloss is 0.4989368812192846 for robust method\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logloss, pred = multiClassRobustTest(theta_lr, X_t, y_t, n_class, np.ones((ns_row, 1)))\n",
        "acc = computeAcc(pred, y_t)\n",
        "print(f'Acc is {acc} and logloss is {logloss} for LR method')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iQzBZdqLszd",
        "outputId": "ef9cd08f-dd6f-481d-bc4a-428821ea4924"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc is 1.0 and logloss is 0.09988054970933052 for LR method\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logloss, pred = multiClassRobustTest(theta_iw, X_t, y_t, n_class, np.ones((ns_row, 1)))\n",
        "acc = computeAcc(pred, y_t)\n",
        "print(f'Acc is {acc} and logloss is {logloss} for IW method')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGbDeVYl24-Y",
        "outputId": "e1361bbf-15ff-4c99-a045-d179a010875d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc is 0.96875 and logloss is 0.17910794400844812 for IW method\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ojdYcUtMzgZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}